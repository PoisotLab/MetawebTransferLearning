{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Step 1 - name matching\n",
    "\n",
    "The names in the parts of the datasets are using different versions of the\n",
    "taxonomy, and so we will reconcile everything using GBIF. The process is\n",
    "entirely automated, and required no human decision. We rely on a mix of strict\n",
    "matching, and then search through synonyms."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The scripts are going to require a number of folders, so we will create them\n",
    "here. Note that it is assume you will download the `data/` folder, and the\n",
    "`Project.toml`. More details on Julia package management can be found on the\n",
    "`Pkg.jl` website, [https://pkgdocs.julialang.org/v1/]."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "if !isdir(\"data\")\n",
    "    error(\"You need to download the data folder from https://github.com/PoisotLab/MetawebTransferLearning/tree/main/data\")\n",
    "end\n",
    "\n",
    "if !isfile(\"Project.toml\")\n",
    "    error(\"You need to download the project file from https://github.com/PoisotLab/MetawebTransferLearning/blob/main/Project.toml\")\n",
    "end\n",
    "\n",
    "isdir(\"artifacts\") || mkdir(\"artifacts\")\n",
    "isdir(\"figures\") || mkdir(\"figures\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dependencies\n",
    "\n",
    "Nothing here is out of the extroardinary - we do some processing on multiple\n",
    "threads, and use `ProgressMeter` to provide visual feedback on the time to\n",
    "completion."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Phylo\n",
    "using GBIF\n",
    "using DataFrames\n",
    "using CSV: CSV\n",
    "using ProgressMeter\n",
    "using DelimitedFiles\n",
    "using Base.Threads"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cleaning the European metaweb"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This step is extracting the species names from the species list in the\n",
    "European metaweb - we will need to go through two steps: removing the\n",
    "non-mammals by filtering on the species code, and then extracting the names\n",
    "these species were given in the European metaweb."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "metaweb_names = String.(readdlm(joinpath(\"data\", \"Spp_Id.txt\")))\n",
    "mammal_positions = findall(startswith(\"M\"), metaweb_names[:, 1])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This line specifically is going to create the list of mammal names that are\n",
    "recorded in the European metaweb. As such, this is the first one we will\n",
    "clean."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mammal_names = metaweb_names[mammal_positions, 2]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To ensure that we can join dataframes together, we will create a data frame\n",
    "with the species code, the name and ID of the matched species in the GBIF\n",
    "backbone, and finally, a flag to check that the two names are equal. This flag\n",
    "is useful for manual inspection; most often, names are unequal because the\n",
    "European metaweb uses deprecated taxonomic names."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that we are actually creating *one* dataframe per thread. This avoids\n",
    "several threads pushing to the same position at the dataframe, which always\n",
    "results in an error. There are, of course, other ways to avoid the issue, but\n",
    "this one is simple and takes a single line to reconcile."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "gbif_cleanup_components = [\n",
    "    DataFrame(; code=String[], gbifname=String[], gbifid=Int64[], equal=Bool[]) for\n",
    "    i in 1:nthreads()\n",
    "]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the dataframe stores in place, we can distribute the name reconciliation\n",
    "on the different threads. The core of this loop is a search on the GBIF web\n",
    "API; this step is therefore going to be limited by the responses of GBIF. Note\n",
    "that although we use unstrict search (to allow for synonyms, etc), we restrict\n",
    "the search to *Mammalia*."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "p = Progress(length(mammal_names))\n",
    "@threads for i in 1:length(mammal_names)\n",
    "    cname = replace(mammal_names[i], '_' => ' ')\n",
    "    try\n",
    "        tax = GBIF.taxon(cname; strict=false, class=\"Mammalia\")\n",
    "        push!(\n",
    "            gbif_cleanup_components[threadid()],\n",
    "            (mammal_names[i], tax.species[1], tax.species[2], cname == tax.species[1]),\n",
    "        )\n",
    "    catch\n",
    "        continue\n",
    "    end\n",
    "    next!(p)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data frames are now filled, and we need to bring them together in a single one\n",
    "-- this is a one-line call to `vcat`, as we simply need to stack them\n",
    "vertically."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "gbif_cleanup = vcat(gbif_cleanup_components...)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clean the GBIF matches"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Names in the European metaweb that are not matched are removed as a\n",
    "precaution:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "select!(gbif_cleanup, Not(:equal))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We finally rename the columns, to facilitate the joins later in the pipeline:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "rename!(gbif_cleanup, :code => :metaweb)\n",
    "rename!(gbif_cleanup, :gbifname => :name)\n",
    "rename!(gbif_cleanup, :gbifid => :id)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We save the file in the `artifacts` folders, as a csv:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "CSV.write(joinpath(\"artifacts\", \"metaweb_gbif.csv\"), gbif_cleanup)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cleaning the phylogeny"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The phylogeny suffers from much the same problem as the European metaweb. We\n",
    "will therefore use much of the same solution to fix the names."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first step is to read the tree as a nexus file using `Phylo`, and to get\n",
    "rid of all the internal nodes -- we are only interested in the leaves of the\n",
    "tree."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tree = open(parsenexus, joinpath(\"data\", \"mammals.nex\"))[\"*UNTITLED\"]\n",
    "treenodes = [n.name for n in tree.nodes if !startswith(n.name, \"Node \")]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This step is a little dirty. Remember that data cleaning is a sin eater, and\n",
    "we need to eat other people's data sins at some point. This is this point.\n",
    "Essentially, we will iterate through the rows of the cleaned metaweb names,\n",
    "and match on either `Gen sp` or `Gen_sp`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is an edge case that was not solved automatically, and so the *S.\n",
    "musicus* species gets its correct tree name manually. This name is caught\n",
    "correctly by *e.g.* `NCBITaxonomy.jl`, but this introduced another dependency\n",
    "in the project, and requires another re-harmonization with the GBIF backbone."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "n = []\n",
    "for row in eachrow(gbif_cleanup)\n",
    "    if replace(row.name, ' ' => '_') in treenodes\n",
    "        push!(n, replace(row.name, ' ' => '_'))\n",
    "    elseif row.metaweb in treenodes\n",
    "        push!(n, row.metaweb)\n",
    "    elseif row.name == \"Spermophilus musicus\"\n",
    "        push!(n, \"Spermophilus_pygmaeus\")\n",
    "    else\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merging the cleaned names together"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because the positions in `n` match with the row indices in the cleaned metaweb\n",
    "names, we add this object as a column:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "gbif_cleanup.upham = n"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We finally save this new file as another artifact. Generally, we will err on\n",
    "the side of caution and save multiple files with redudancy in them. This is\n",
    "good practice, and it allows for easier data dumpster diving if something goes\n",
    "wrong."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "CSV.write(joinpath(\"artifacts\", \"names_metaweb_tree_gbif.csv\"), gbif_cleanup)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.2",
   "language": "julia"
  }
 },
 "nbformat": 4
}
