using PhyloNetworks
using ProgressMeter
using LinearAlgebra
using EcologicalNetworks
using DelimitedFiles
using Random
using StatsPlots
using StatsBase
using Statistics
using SparseArrays
using EcologicalNetworksPlots
using CSV
using DataFrames
using GBIF
using Base.Threads
using StatsBase
using StatsModels
using Distributions

theme(:mute)
default(; frame=:box)
Random.seed!(01189998819991197253)

# Get ancillary functions
include("lib/pwar.jl")

# Read the tree
tree_net = readTopology(joinpath("data", "mammals.newick"));

# Read the names
namelist = DataFrame(CSV.File(joinpath("artifacts", "names_metaweb_tree_gbif.csv")))

# Prepare the metaweb
eurometa = readdlm(joinpath("artifacts", "europeanmetaweb.csv"), ',', String)
mwspecies = unique(eurometa)
M = UnipartiteNetwork(zeros(Bool, length(mwspecies), length(mwspecies)), mwspecies)
for i in 1:size(eurometa, 1)
    M[eurometa[i, :]...] = true
end

# Read the metaweb
eurometa = readdlm(joinpath("artifacts", "europeanmetaweb.csv"), ',', String)
mwspecies = unique(eurometa)
M = UnipartiteNetwork(zeros(Bool, length(mwspecies), length(mwspecies)), mwspecies)
for i in 1:size(eurometa, 1)
    M[eurometa[i, :]...] = true
end

# Get the correct rank to cut the matrix at
rnk = rank(Array(M.edges))
eig = svd(M).S[1:rnk]
neig = eig./sum(eig)

# Plot the eigenvalues
scatter(eig, lab="", dpi=600, size=(500,500))
xaxis!("Dimension", (1,38))
yaxis!("Eigenvalue", (0, 40))
savefig("figures/screeplot.png")

# Plot the variance explained
scatter(cumsum(neig), lab="", dpi=600, size=(500,500))
for ve in [0.5, 0.6, 0.7, 0.8, 0.9]
    i = findfirst(cumsum(neig) .> ve)
    x = cumsum(neig)[i]
    plot!([0, i, NaN, i, i], [x, x, NaN, x, 0], lab="", c=:grey, ls=:dash)
end
xaxis!("Dimension", (1,38))
yaxis!("Cumulative variance explained", (0, 1.0))
savefig("figures/varexplained.png")

# Partition the metaweb into two latent subspaces, using enough dimensions
# to explain 60% of variance
L, R = rdpg(M, 12)

# Get the correct threshold
A = adjacency(M)
thresholds = LinRange(extrema(L * R)..., 500)
tp = zeros(Float64, length(thresholds))
tn = similar(tp)
fp = similar(tp)
fn = similar(tp)
for (i, t) in enumerate(thresholds)
    PN = (L * R) .>= t
    tp[i] = sum((A) .& (PN)) / sum(A)
    tn[i] = sum((.!(A)) .& (.!(PN))) / sum(.!(A))
    fp[i] = sum((.!(A)) .& (PN)) / sum(.!(A))
    fn[i] = sum((A) .& (.!(PN))) / sum(A)
end

Y = tp .+ tn .- 1
maxY, posY = findmax(Y)
threshold = thresholds[posY]

plot(thresholds, Y, lw=0.0, fill=(0, 0.2), lab="", dpi=600, size=(500,500))
scatter!([threshold], [maxY], lab="", c=:darkgrey, msw=0.0)
yaxis!("Youden's J", (0,1))
xaxis!("Cutoff", extrema(L*R))
savefig("figures/optimalcutoff.png")

# Plot the subspaces
plot(
     heatmap(L, c=:Greys, frame=:none, cbar=false),
     heatmap(R', c=:Greys, frame=:none, cbar=false),
     dpi=600,
     size=(500,400)
    )
savefig("figures/subspaces.png")

# Get the species names
treeleaves = tipLabels(tree_net)

canada = DataFrame(CSV.File(joinpath("artifacts", "iucn_gbif_names.csv")))
cancodes = replace.(unique(filter(!ismissing, canada.gbifname)), " " => "_")
tree_cleanup = DataFrame(CSV.File(joinpath("artifacts", "upham_gbif_names.csv")))

csp = dropmissing!(DataFrame(; gbifname=canada.gbifname))
csp = dropmissing(leftjoin(csp, tree_cleanup; on=:gbifname))

canmammals = unique(csp.code)

# We start getting a species pool to infer the traits
pool = DataFrame(; tipNames=canmammals)

metaweb_can_names = replace.(M.S, " " => "_")
metaweb_can_names âˆ© canmammals

#### I am missing some species from the metaweb
filter((x) -> (x) âˆ‰ (metaweb_can_names âˆ© treeleaves), metaweb_can_names)

traitframe = DataFrame(; tipNames=treeleaves)

matching_tree_reconstruction = DataFrame(;
    tipNames=treeleaves, nodeNumber=range(1, length(treeleaves); step=1)
)

# Automatically name the traits with L or R prefixes

leftnames = "L".*string.(1:size(L,2))
traits_L = DataFrame(L, Symbol.(leftnames))
traits_L[!, "tipNames"] = metaweb_can_names

rightnames = "R".*string.(1:size(R,1))
traits_R = DataFrame(R', rightnames)
traits_R[!, "tipNames"] = metaweb_can_names

# Use a single dataframe for the traits
traits = leftjoin(traitframe, traits_L; on=:tipNames)
traits = leftjoin(traits, traits_R; on=:tipNames)

# Imputed traits dataframe
imputedtraits = DataFrame(;
    tipNames=[treeleaves; fill(missing, tree_net.numNodes - tree_net.numTaxa)]
)

# Imputation loop!
for coord in 1:size(L,2)
    for prefix in ["L", "R"]
        @info "Reconstructing $(prefix)$(coord)"
        lower, upper, mean_trait = leaf_traits_reconstruction(
            traits[!, ["$(prefix)$(coord)", "tipNames"]],
            tree_net
        )
        imputedtraits[!, "$(prefix)$(coord)_low"] = lower
        imputedtraits[!, "$(prefix)$(coord)_up"] = upper
        imputedtraits[!, "$(prefix)$(coord)_mean"] = mean_trait
    end
end

# We save the reconstructed values
canadian_rec = innerjoin(dropmissing(imputedtraits), pool; on=:tipNames)

# Get the left and right subspaces (and the lower and upper values)
ð“ = Array(canadian_rec[!, leftnames.*"_mean"])
ð“‡ = transpose(Array(canadian_rec[!, rightnames.*"_mean"]))

plot(
     heatmap(ð“, c=:Greys, frame=:none, cbar=false),
     heatmap(ð“‡', c=:Greys, frame=:none, cbar=false),
     dpi=600,
     size=(500,400)
    )
savefig("figures/imputed-subspaces.png")

ð“â‚—= Array(canadian_rec[!, leftnames.*"_low"])
ð“‡â‚— = transpose(Array(canadian_rec[!, rightnames.*"_low"]))

ð“áµ¤ = Array(canadian_rec[!, leftnames.*"_up"])
ð“‡áµ¤ = transpose(Array(canadian_rec[!, rightnames.*"_up"]))

â„’ = Matrix{Uniform}(undef, size(ð“))
for i in eachindex(â„’)
    â„’[i] = Uniform(ð“â‚—[i], ð“áµ¤[i])
end

â„› = Matrix{Uniform}(undef, size(ð“‡))
for i in eachindex(â„›)
    â„›[i] = Uniform(ð“‡â‚—[i], ð“‡áµ¤[i])
end

draws = 20_000

ð‹ = [rand.(â„’) for i in 1:draws]
ð‘ = [rand.(â„›) for i in 1:draws]

# We get the thresholded networks here
Ns = [(ð‹[i] * ð‘[i]) .> threshold for i in 1:length(ð‹)]
P = UnipartiteProbabilisticNetwork(
    reduce(.+, Ns) ./ draws, replace.(canadian_rec.tipNames, "_" => " ")
)

# Deterministic version
N = UnipartiteNetwork(ð“*ð“‡ .>= threshold, replace.(canadian_rec.tipNames, "_" => " "))

sort(interactions(P); by=(x) -> x.probability, rev=true)

histogram([x.probability for x in interactions(P)])

sporder = sortperm(vec(sum(adjacency(P); dims=2)))
h1 = heatmap(adjacency(P)[sporder, sporder], c=:YlGnBu, frame=:none, cbar=false, dpi=600, size=(500, 500), aspectratio=1)

sporder = sortperm(vec(sum(adjacency(M); dims=2)))
h2 = heatmap(adjacency(M)[sporder,sporder], c=:Greys, frame=:none, cbar=false, dpi=600, size=(500,500), aspectratio=1)

plot(h2, h1, size=(1000,500))
savefig("figures/adjacencymatrices.png")

output = DataFrame(from = String[], to = String[], score = Float64[], pair = Bool[], int = Bool[])
for int in interactions(P)
    pair = (int.from in species(M)) & (int.to in species(M))
    mint = pair ? M[int.from, int.to] : false
    push!(output, (
        int.from, int.to, int.probability, pair, mint
    ))
end
sort!(output, [:score, :from, :to], rev=[true, false, false])

# Save the basic network (no corrections)
CSV.write("artifacts/canadian_uncorrected.csv", output)

# Exploration of the relationship between subspaces and network properties
kout = degree(P; dims=1)
kin = degree(P; dims=2)
ordered_sp = replace.(canadian_rec.tipNames, "_" => " ")

scatter(ð“[:,1], [kout[s]/richness(P) for s in ordered_sp], dpi=600, size=(500, 500), lab="")
xaxis!("Position in the left subspace", extrema(vcat(ð“‡', ð“)))
yaxis!("Probabilistic generality", (0,1))
savefig("figures/left-gen.png")

scatter(ð“‡'[:,1], [kin[s]/richness(P) for s in ordered_sp], dpi=600, size=(500, 500), lab="", legend=:bottomright)
xaxis!("Position in the right subspace", extrema(vcat(ð“‡', ð“)))
yaxis!("Probabilistic vulnerability")
savefig("figures/right-vuln.png")

# Corrections assuming
# - if species don't interact in Europe, no interaction in Canada
# - if species interact in Europe, interaction in Canada
N = copy(P)
shared_species = filter(s -> s in species(M), species(P))
for s1 in shared_species
    for s2 in shared_species
        N[s1,s2] = M[s1,s2] ? 1.0 : 0.0
    end
end
SparseArrays.dropzeros!(N.edges)
simplify!(N)

# Final metaweb
final = DataFrame(from = String[], to = String[], score = Float64[])
for int in interactions(N)
    push!(final, (int.from, int.to, int.probability))
end
sort!(final, [:score, :from, :to], rev=[true, false, false])

# Save the corrected network 
CSV.write("artifacts/canadian_corrected.csv", final)
